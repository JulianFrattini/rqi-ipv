---
title: "Bayesian Analysis of the Impact of Passive Voice on Missing Domain Objects"
author: "Julian Frattini"
date: '2023-12-04'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(patchwork)

library(ggdag)

library(brms)
library(marginaleffects)
source("../util/model-eval.R")
```

This document contains a Bayesian analysis of the hypotheses investigated by Femmer et al.^[Femmer, H., Kučera, J., & Vetrò, A. (2014, September). On the impact of passive voice requirements on domain modelling. In Proceedings of the 8th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (pp. 1-4).].

## Data Loading

Start by loading the prepared data.

```{r data-loading}
source("../util/data-loading.R")
d <- load.data()

# print the data to ensure that all variables have the correct type
str(d)
```


## Bayesian Analysis

We perform the final step of the statistical causal inference framework by Siebert^[Siebert, J. (2023). Applications of statistical causal inference in software engineering. Information and Software Technology, 107198.], i.e., the estimation step, based on the two previous modeling and identification steps as documented in (the causal assumptions notebook)[./causal-assumptions.Rmd].

### Formula

First, we define a regression model that predicts the number of missing domain objects based on a set of predictors identified during the causal assumptions. The most eligible probability distribution of the response variable is a Binomial distribution, since the response variable is a whole number bounded by the expected number of domain objects.

```{r formula}
formula <- (objects.missing | trials(objects.expected) ~ 
              1 + # standard success in the task of finding actors
              passive + # impact of the use of passive voice
              (1|RID) + # impact of the difficulty of the individual requirements
              (1|PID) + # impact of the skill of individual participants
              ExpREAca + # impact of the participants academic experience in RE
              ExpREInd) # impact of the participants industrial experience in RE

get_prior(formula, family=binomial, data=d)
```

### Priors

For each of the selected predictors, we select an uninformative prior distribution that encodes our previous knowledge about the causal phenomenon.

```{r priors}
priors <- c(prior(normal(-1.5, 1), class = Intercept),
            prior(normal(0, 1), class = b),
            prior(weibull(2, 1), class = sd, coef = Intercept, group = RID),
            prior(weibull(2, 1), class = sd, coef = Intercept, group = PID),
            prior(exponential(1), class = sd))
```

To assert that the priors are feasible, we sample from the priors without training the Bayesian model with the data.

```{r model-prior, message=FALSE, warning=FALSE}
m.prior <-
  brm(data = d, family = binomial, formula, prior = priors,
    iter = 4000, warmup = 1000, chains = 4, cores = 4,
    seed = 4, sample_prior="only",
    file = "fits/m.objects.prior"
  )
```

We visualize the results to graphically check the priors eligibility.

```{r prior-predictive-check}
ndraws <- 400
brms::pp_check(m.prior, type="bars", ndraws=ndraws)
```

The distribution of the predicted values for the response variable ($y_{rep}$) overlaps with the actually observed response variable distribution ($y$), confirming the feasibility of the priors.

### Training

Given the feasibility of the priors, we train the regression model. This process updates the prior distributions of all predictor coefficients.

```{r model, message=FALSE, warning=FALSE}
m <-
  brm(data = d, family = binomial, formula, prior = priors,
    iter = 4000, warmup = 1000, chains = 4, cores = 4,
    seed = 4,
    file = "fits/m.objects"
  )
```

To assert that the training process worked correctly, we plot the Markov chains.

```{r mcmc-plot}
plot(m)
```


All chains seem fine. Further, we sample from the posterior distributions similar to the prior predictive check.

```{r posterior-predictive-check}
brms::pp_check(m, type="bars", ndraws=ndraws)
```

The distribution of the predicted values for the response variable ($y_{rep}$) still overlaps with the actually observed response variable distribution ($y$). Additionally, the distribution shrunk around the actually observed values, which indicates that the model has improved its predictive power through training.

### Evaluation

Finally, we evaluate the trained model. An initial glance at the predictor coefficients shows their posterior distribution.

```{r summary}
summary(m)
```

These coefficient distributions do not show the full uncertainty of the impact of each predictor, though. Therefore, we plot the marginal effect of relevant predictors. The most relevant predictor is the treatment of the experiment, the use of passive voice. The plot shows that the use of passive voice (`passive=TRUE`) slightly increases the likelihood of missing a domain object, but since the confidence intervals overlap, this effect is not significant.

```{r marginal-passive}
marginal.passive <- conditional_effects(m, effects="passive")
marginal.passive
```

Export the results to a `CSV` sheet to enable a figure of all three marginal plots in the end.

```{r marginal-passive-export}
marginal.passive.values <- data.frame(marginal.passive[1]) %>% 
  select(passive.passive, passive.estimate__, passive.lower__, passive.upper__) %>% 
  rename(all_of(c(
    passive = "passive.passive",
    estimate = "passive.estimate__", 
    ci.lower = "passive.lower__", 
    ci.upper = "passive.upper__"))) %>% 
  mutate(
    response = "objects"
  ) %>% 
  write_csv(file = "../../data/results/marginal-objects.csv")
```

Finally, we can evaluate the model numerically. This is similar to the marginal effect, but can be summarized to the amount of times (in percent) that the use of passive voice produces fewer (-1), equal (0), or more (+1) missing domain objects.

```{r evaluation}
evaluate.model(m)
```

The evaluation also supports that the effect of passive voice on the number of missing domain objects is not significant. 